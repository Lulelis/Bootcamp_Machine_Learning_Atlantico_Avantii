{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e7faa4a",
   "metadata": {},
   "source": [
    "## > ***Atividade_01 | Diagnóstica de Machine Learning***\n",
    "\n",
    "### - Conceitos Básicos de Machine Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b59b906",
   "metadata": {},
   "source": [
    "### **1** -) Explique com suas palavras o que é Machine Learning:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6288dc",
   "metadata": {},
   "source": [
    "#### > Consigo definir Machine Learning ou aprendizado de máquina acerca do primeiro conceito que me vem à mente como: campo da inteligência artificial que usa algoritmos e modelos estatísticos para aprender padrões a parti de dados, uma tentativa de aprendizado por parte da I.A em replicar algum comportamento Humano, na maioria das vezes um comportamento executado de forma manual e repetitiva. Ou seja, a inteligência artificial replica padrões e aprende, sejam elas por formas: supervisionada, não supervisionada e reforço, alguma tarefa que envolva muita troca de informações (geralmente o conjunto de dados) e retorne a quem está coordenando o resultado da tarefa. Nesse sentido, seja ele qual for o aprendizado, acontece sem que o modelo seja explicitamente programado para cada tarefa, mas sim ajustando parâmetros com base em exemplos. Portanto, seja o algoritmo e o problema que se quer solucionar, o objetivo é generalizar, aplicar o que se aprendeu com a entrada de dados novos, não apenas memorizando os antigos.\n",
    " \n",
    " #### Como define Alan Turing: “Um computador pode ser chamado de inteligente se conseguir induzir uma pessoa a pensar que é um ser Humano”. Tendo em vista o que foi levantado, a máquina até então, só consegue fazer coisas que a gente faria: e se nós conseguimos somos considerados inteligentes e elas também são, por conseguinte.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3ae3d",
   "metadata": {},
   "source": [
    "### **2** -)Explique o conceito de conjunto de treinamento, conjunto de validação e conjunto de teste em machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d5a18",
   "metadata": {},
   "source": [
    "#### > Pode-se definir o conceito pensando especificamente neste caso, em um problema de análise supervisionada que são basicamente problemas de inferências. Pensando em dados tabulares (dados estruturados/dataframes), têm-se dados com rótulos (labels/features/Colunas) e a forma mais utilizada de se simular uma condição de dados que nunca foram vistos é a separação dos dados que temos em nossa posse. Os dados já rotulados são separados na base onde o algoritmo será treinado e na base onde o algoritmo será confrontado com a simulação da vida real. Por exemplo: Dados provenientes de transações bancárias com rótulos de fraude e “não fraude”.  Dado um problema de classificação, se precisa aprender com esses dados de forma que novas transações possam ser classificadas. \n",
    "#### Desse modo, se tem a separação dos dados (pensando em um problema de classificação> aprendizado supervisionado no qual se trabalha majoritariamente com dados rotulados e tabulares), em:\n",
    "#### **Treino:** Centro do modelo supervisionado. Onde os algoritmos efetivamente serão treinados para se tornarem generalizáveis com dados não vistos, ou seja, é na parte do conjunto de dados separados para treino, nesta etapa de treinamento, preza-se por conseguir ter uma boa performance em dados futuros. Acrescido a isso, é aonde o modelo ajusta seus parâmetros. Por exmeplo, aqui, o risco é o **overfiting** (*sobreajuste*), quando o modelo 'decora' o conjunto de dados e aprende demais os detalhes do treino e não generaliza.\n",
    "#### **Validação:** Ou desenvolvimento. Etapa onde a base será validada, os resultados dessa base serão utilizados para saber qual o melhor modelo. Nesse sentido, com os modelos treinados lá na base de treino previamente separada. Realiza-se as predições nesta etapa para coletas as métricas. Portanto, além de comparar os modelos, também é usada para **ajustar hiperparâmetros** (como profundidade de árvore, taxa de aprendizado, etc)depende tanto do tipo de problema quanto do algoritmo escolhido. Por exemplo, em uma árvore de decisão, os hiperparâmetros podem ser a profundidade máxima ou o número mínimo de amostras por folham, em uma rede neural pode ser o número de camadas, neurônios ou taxa de aprendizado. É necessário ressaltar que cada problema a ser resolvido, exige atenção e critérios distintos, como em: classificação, regressão e até segmentação.\n",
    "#### **Teste:** Esta divisão do conjunto de dados só será utilizada no final sem que ocorra nesta parte de conjunto de dados, teste de performance de algoritmos ou hiperparametrização. Portanto, no conjunto de teste é uma simulação de implementação dos modelos para que eles validem de acordo com suas configurações e testagem feitas anteriormente. Quando se chega no melhor modelo de algoritmo se usa a base de teste para saber como foi a performance. ***não se pode VALIDAR em base de teste**! Resumidamente, essa separação em teste da base de dados serve como uma \"prova final\" para medir a capacidade de generelização. Deve ser usado uma única vez, depois que todas as escolhas de modelo e hiperparâmetros já foram feitas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aa9297",
   "metadata": {},
   "source": [
    "### **3** -)Explique como você lidaria com dados ausentes em um conjunto de dados de treinamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1057dc1",
   "metadata": {},
   "source": [
    "#### Resumidamente, penso que é necessário entender a natureza dos dados faltantes para se traçar algum plano de ação, sejam eles: exclusão, imputação por estatísticas de tendência central ou por modelagem de algoritmos de aprendizado (knn, hot deck, Iterativa Imputer, modelos baseados em regressões).\n",
    "#### Assim, os dados faltantes ocorrem quando nenhum valor é armazenado em determinada variável em uma instância de conjunto de dados (pensando em um dataframe, dados tabulares/retangulares).\n",
    "#### Portanto a identificação e o entendimento dos padrões de dados ausentes são etapas cruciais na análise de dados, pois podem afetar diretamente as conclusões e insights obtidos. Mesmo que existam várias técnicas a serem empregas com base nas características dos conjunto de dados e a relação existente entre as variáveis, não existem soluções únicas, dependem da proporção dos dados ausentes ocupadas em colunas de atributos ou nas tuplas de linhas bem como a relevância em si daqueles dados ausentes representam no conjunto de dados trabalhado.\n",
    "\n",
    "#### Podem ser classificados em: \n",
    "\n",
    "#### - **MCAR**|Missing completely at Random|(Sem relação com as informações ausentes ou outras variáveis).\n",
    "\n",
    "#### - **MAR** | Missing At Random| Relacionada a outras variáveis modelável com base nas informações disponíveis\n",
    "\n",
    "#### - **MNAR** | Missing Not at Random| Ligada à própria informação faltante. Não explicável por outras variáveis observadas\n",
    "\n",
    "#### Portanto, a primeira estratégia antes de imputar os valores faltantes, seria: Plotar um gráfico de correlação e entender para saber qual tipo pertence, verificando a correlação entre as variáveis e os valores faltantes afim de se comparar com outras variáveis.\n",
    "#### Nesse sentido, os métodos que consigo pensar quando se trata de dados ausentes, são:\n",
    "\n",
    "#### Deleção de Linhas (em última instância, pois quando se está trabalhando com dados, a última solução a ser feita é excluir a linha ou coluna, para que possa não inserir viés de informação na própria informação que se está trabalhando (dados).\n",
    "#### Deleção de Colunas: Evitam a necessidade de análise complexa e da mesma forma que ocorre na deleção de linhas (objetos) pode-se perder informação importante.\n",
    "#### Desse modo, é necessário trabalhar com a abordagem, sem modificar a distribuição dos dados, sem inserir viés.\n",
    "#### Imputação por medidas de tendência central: Os valores ausentes são substituídos pela média, moda ou mediana dos valores conhecidos para aquela variável No entanto, mesmo sendo uma abordagem simples e rápida, mas pode introduzir viés nos dados, especialmente se houver um número significativo de valores ausentes.\n",
    "#### Imputação por Hot-Deck -->  Nessa técnica os valores ausentes são substituídos por valores de observações semelhantes no conjunto de dados. Essa abordagem requer a identificação de observações semelhantes com base em características compartilhadas.\n",
    "#### Imputação por Modelos (Classificação ou Regressão) --> Valores ausentes são estimados com base em um modelo construído usando as demais variáveis do conjunto de dados. Os modelos permitem uma imputação mais precisa, levando em consideração as relações entre as variáveis. No entanto, essa abordagem pode inserir ruídos nos dados, bem como valores não permitidos, de acordo com o modelo escolhido.\n",
    "#### Impurtação Iterativa Imputer--> Faz a imputação iterativa: cada variável é prevista em rodízio usando as demais. Muito flexível, pode usar regressão, random forestm e Bayesin Ridge.\n",
    "\n",
    "\n",
    "**Tabela com o resumo dos principais algoritmos de aprendizadi utilizados na imputação de dados ausentes:**\n",
    "| Médoto  | Como funciona | Vantagens| Limitações | Quando usar|\n",
    "|---------|------------|---------|----------|-----------|\n",
    "| Hot-Deck    | Substitui valores ausentes por observações semelhantes do dataset  | Preserva variabilidade real dos dados |Requer boa definição de “semelhança”; pode ser complexo| Quando existem grupos ou perfis claros (ex.: clientes com características similares)\n",
    "| KNN Imputer    | Usa os k vizinhos mais próximos para estimar valores | Captura relações não lineares; simples de entender|Pode ser lento em datasets grandes; sensível à escala| Quando há correlação forte entre variáveis e dataset não é gigantesco\n",
    "|Regressão (model-based)|Usa modelos (linear, logística, etc.) para prever valores ausentes| Aproveita relações entre variáveis; pode ser preciso| Risco de overfitting; exige bom modelo|Quando a variável ausente tem forte relação com outras|\n",
    "| Iterative Imputer (scikit-learn)|Faz imputação iterativa: cada variável é prevista em rodízio usando as demais|Muito flexível; pode usar regressão, random forest, Bayesian Ridge|Computacionalmente pesado; risco de ruído|Quando há muitas variáveis correlacionadas e dataset é grande/complexo\n",
    "#### Por fim, é necessário acrescentar que qualquer técnica de imputação empregada pode alterar a distribuição dos dados e, portanto, devem ser avaliadas com cuidado devido ao risco viés de análises geradas provenientes dos dados que acabam se tornando informações enviesadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443bfe24",
   "metadata": {},
   "source": [
    "### **4** -) O que é uma matriz de confusão e como ela é usada para avaliar o desempenho de um modelo preditivo?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105129e7",
   "metadata": {},
   "source": [
    "#### Matriz de confusão é uma métrica de avaliação do desempenho dos modelos de classificação, ou seja, a taxa de acerto no qual o modelo submetido a treino apresentará no sucesso em determinar a predição daqueles dados.  \n",
    "#### Uma tabela que descreve a atuação do classificador (pensando em um exemplo de aprendizado supervisionado para problemas de classificação) de acordo com a quantidade de verdadeiros positivos (True Positive/TP), falsos positivos (False Positive/FP) e falsos negativos (False Negative). Assim, um classificador tem boa performance quando a diagonal principal da matriz de confusão tem valores altos e a diagonal secundária apresenta valores baixos. \n",
    "#### Assim, uma matriz de confusão não só mostra a taxa de acerto geral, mas também revela onde o modelo erra(confundido muito positivos com negativos)\n",
    "Conforme imagem disponível abaixo:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"confusion_matrix.png\" alt=\"Confusion Matrix\" width=\"700\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545dd922",
   "metadata": {},
   "source": [
    "#### Dessa forma, na diagonal principal: É aonde se espera no qual apresente valores grandes (pois, serão os classificados corretamente), já na diagonal secundária, se esperam os valores pequenos (classificador binário)\n",
    "#### ***TP/True Positive***. Valores verdadeiramente positivos. É a quantidade de casos positivos classificados corretamente.\n",
    "#### ***TN/True Negative**. Número ou quantidade de casos negativos classificados corretamente.\n",
    "#### ***FP/False Positive***. Número ou quantidade de casos negativos classificados incorretamente como positivos.\n",
    "#### ***FN/False Negative***. Número ou quantidade de casos positivos  classificados incorretamente como negativos.\n",
    "\n",
    "#### Portanto, as métricas derivadas da matriz de confusão, são utilizadas para calcularem métricas como: **acurácia**, **previsão**, **recall(sensibilidade)**, e **especificidade**, que ajudam a avaliar o modelo de uma forma mais robusta & completa.\n",
    "#### Por exemplo: em um modelo de detecção de fraude, um **false negative(FN)**(fraude não detectada) é muito mais grave do que um **falso posito/FP** (transação legítima marcada como fraude). A matriz de confusão ajudam a enxergar esse tipo de impacto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3c96e",
   "metadata": {},
   "source": [
    "### **5** -)Em quais áreas (tais como construção civil, agricultura, saúde, manufatura, entre outras) você acha mais interessante aplicar algoritmos de machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934a0054",
   "metadata": {},
   "source": [
    "#### Acredito que dentre as várias áreas de destaque na aplicação de aprendizado de máquina, consigo citar algumas, dentre elas:\n",
    "\n",
    "#### •\tAutomatização de carros com Machine Learning para leitura de placas, para aproximação de veículos próximos\n",
    "#### •\tAssistentes Virtuais --> Transformação da voz em texto/Speech to text\n",
    "#### •\tSistemas de recomendação --> De Streaming, google, compras e músicas| Simples e complexos, depende do algoritmo e do investimento. O sistema quer que a pessoa continue \"Scrollando\" continue comprando, assistindo, enfim, retendo o consumidor no aplicativo para gerar dados e angariar cliques\n",
    "#### •\tAnálise de Churn --> Cliente vai sair ou não dos serviços contratados. Então é a predição de quando ou se o consumidor sairá do serviço contratado-Saber com certa precisão se a pessoa está tendendo encerrar o seu contrato com a empresa e se conseguirá resgatá-la de volta\n",
    "\n",
    "#### •\tClassificação de raio-x --> Diagnósticos de pacientes para doenças\n",
    " \n",
    "#### Por fim ,também ressalto que as Aplicações de Machine Learning Simples são bastante interessantes de serem trabalhadas, uma que o foco, acredito, é  basicamente Trabalhar sempre pensando em resolver problemas!\n",
    " \n",
    "#### Segmentação de Recomendação--> Tentar achar similaridades entre produtos e clientes que compram esses produtos para tentar prever qual será o produto que ele comprará, por predição;\n",
    "#### Segmentação de Clientes -->Trabalhar no sentido dos Bancos usam para realizar segmentação dos perfis de clientes. Pensando na detecção de anomalias em aprendizado supervisioanda\n",
    "#### Previsão de Demanda e Previsão de Estoque -->Fazer a empresa economizar, melhorar as metodologias já usuais;\n",
    "#### Setor de Manufatura --> Manutençãi preditiva de máquinas, otimização de linhas de produção, controle de qualidade utilizando *visão computacional*;\n",
    "#### Setor de Saúde--> Auxílio na classificação de imagens de doenças e raio-x| Prevenção de Readmissão --> Por exemplo: Quando um hospital dá  alta e a pessoa retorna, é um gasto nessa readmissão, dessa forma com ML se consegue atenuar isso\n",
    "#### Em resumo, o aprendizado de máquina é mais interessante quando aplicado para resolver problemas reais, seja aumentando eficiência, reduzindo custos ou melhorando a qualidade de vida das pessoas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb81fb",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
